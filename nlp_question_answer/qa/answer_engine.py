# coding=UTF-8

"""
Created on 10/30/14

@author: 'johnqiao'

Provides document analysis and answer extraction functions and classes.

Copyright (C) 2012 Brian Wesley Baugh

"""

from __future__ import division
import math

import nltk
from nltk.corpus import wordnet

import indexer


class AnswerEngine(object):
    """Provides the methods to turn a query into a list of answers.

    Attributes:
        query: The direct query string from the user.
        start: The number of pages to offset from the beginning of the
            page list returned by the index.
        num_top: The number of pages (from the top of the ranked list
            of pages (sorted by similarity) returned by the index) to
            extract answers from.
            Combined withe the start-argument, this allows for paging
            through the results by only looking at a certain number of
            pages at a time.
        lch: The Leacock-Chodorow Similarity measurement. Used to
            determine if two WordNet senses (synsets) are related.
            The default value has been empirically determined to
            provide good results, though it may be fine-tuned. This
            attribute is a float.
        ir_query: The regularized string sent to the IR index. It is
            a list of tokens.
        ir_query_tagged: The IR query string that has each possible
            WordNet sense associated with each tokenized word. This
            can then be displayed to the user, and in a future update
            this class could use the disambiguated word sense to
            improve the answer extraction process. This attribute is a
            list of tuples. Each tuple contains the word in the first
            position, followed by a list of WordNet Synset objects.
        num_pages: The number of pages returned by the IR search.
        pages: Ranked list of Page objects returned by the IR search.
            The number of pages is usually less than num_pages unless
            during the call to __init__() the value for num_top is
            greater than or equal to num_pages.
        answers: List of Answer objects generated by the class.
    """

    def __init__(self, index, query, start=0, num_top=10, lch=2.16):
        """Inits AnswerEngine by querying the IR module to get Page objects.

        Args:
            index: An indexer.Index object, which represent the IR system.
            query: The direct query string from the user.
            start: The number of pages to offset from the beginning of the
                page list returned by the index.
            num_top: The number of pages (from the top of the ranked list
                of pages (sorted by similarity) returned by the index) to
                extract answers from.
                Combined withe the start-argument, this allows for paging
                through the results by only looking at a certain number of
                pages at a time.
            lch: The Leacock-Chodorow Similarity measurement. Used to
                determine if two WordNet senses (synsets) are related.
                The default value has been empirically determined to
                provide good results, though it may be fine-tuned. This
                argument should be a float.
        """
        self.query = query
        self.start = start
        self.num_top = num_top
        self.lch = lch
        self.answers = None
        # Candidate Document Selection
        self.ir_query = indexer.regularize(indexer.tokenizer.tokenize(query))
        self.ir_query_tagged = None
        page_sim = index.ranked(self.ir_query)
        self.num_pages = len(page_sim)
        # Reduce number of pages we need to get from disk
        page_sim = page_sim[start:num_top]
        page_ids, similarity = zip(*page_sim)
        # Retrieve the Page objects from the list of Page.IDs
        self.pages = index.get_page(page_ids)
        # Tell each page the value of its similarity score
        for page, sim in zip(self.pages, similarity):
            page.cosine_sim = sim

    def _analyze_query(self):
        """Creates the ir_query_tagged attribute.

        The ir_query_tagged attribute is a list of tuples. Each tuple
        contains the word in the first position, followed by a list of
        Synset objects from WordNet.
        """
        tagged = nltk.pos_tag(self.ir_query)
        ir_query_tagged = []
        for word, pos in tagged:
            pos = {
                pos.startswith('N'): wordnet.NOUN,
                pos.startswith('V'): wordnet.VERB,
                pos.startswith('J'): wordnet.ADJ,
                pos.startswith('R'): wordnet.ADV,
                }.get(pos, None)
            if pos:
                synsets = wordnet.synsets(word, pos=pos)
            else:
                synsets = wordnet.synsets(word)
            ir_query_tagged.append((word, synsets))
        # Add additional special hidden term
        ir_query_tagged.append(('cause', [wordnet.synset('cause.v.01')]))
        self.ir_query_tagged = ir_query_tagged

    def _analyze_pages(self):
        """Performs candidate document analysis and information extraction."""
        for page in self.pages:
            page.tokenize_sentences()

    def related(self, synsets, word2):
        """Check if two words have related synsets."""
        for net1 in synsets:
            for net2 in wordnet.synsets(word2):
                try:
                    lch = net1.lch_similarity(net2)
                except:
                    continue
                # The value to compare the LCH to was found empirically.
                if lch >= self.lch:
                    return True
        return False

    def related_values(self, synsets, word2):
        """Get list of related value between synsets of two words."""
        related = []
        for net1 in synsets:
            for net2 in wordnet.synsets(word2):
                try:
                    lch = net1.lch_similarity(net2)
                except:
                    continue
                related.append(lch)
        return related

    def _extract_answers(self):
        """Extract answers from the pages using all the tagged information.

        This method should be run only after _analyze_pages().
        """
        answers = []
        for page in self.pages:
            page_windows = []
            for para in page.paragraphs:
                for sentence in para.sentence_tokens:
                    # if len(page_windows) == 3:
                    #     break
                    page_windows.append(Answer(page, sentence,
                                        ' '.join(sentence), self))
            answers.extend(page_windows)
        answers = [x for x in answers if x.score > 0]
        answers.sort(key=lambda answer: answer.score, reverse=True)
        # answers.sort(key=lambda answer: answer.page.cosine_sim, reverse=True)
        self.answers = answers

    def get_answers(self):
        """Performs answer extraction after processing the Page documents.

        This method is a convenience method to call the appropriate
        candidate document analysis and other information extraction
        methods in the appropriate order.

        Returns:
            A list of Answer objects representing the answers extracted
            from the internal list of (relevant) Pages. This list is also
            available by accessing the answers attribute of the instance.
        """
        
        self._analyze_query()
        self._analyze_pages()
        self._extract_answers()
        return self.answers


class Answer(object):
    """Represents a single answer."""

    def __init__(self, page, raw_tokens, text, ans_eng):
        """Initializes the Answer object with the Page and answer text.

        Args:
            page: The Page object that the text for this answer is from.
            raw_tokens: List of token strings, such as from a sentence.
            text: The plain-text version of the raw_tokens; for output.
            ans_eng: The AnswerEngine object that was used to generate
                this Answer object.
        """
        self.page = page
        self.text = text
        self._features = None
        self.score = self.get_score(raw_tokens, ans_eng)

    def get_score(self, raw_tokens, ans_eng):
        """Compute a score for this answer so that it may be ranked.

        Args:
            raw_tokens: List of token strings, such as from a sentence.
            ans_eng: The AnswerEngine object that was used to generate
                this Answer object.

        Returns:
            A float between 0 and 1 that represents the confidence that
            this answer is a correct answer for the query specified in
            the AnswerEngine object.
        """
        (
         term_count,
         related,
         causal_match,
         position,
        ) = self._compute_score(raw_tokens, ans_eng)
        if term_count == 0:
            return 0
        position = math.sqrt(sum(position))
        self._features = (
                         self.page.cosine_sim,
                         term_count,
                         sum(related),
                         sum(related) / len(ans_eng.ir_query),  # average
                         causal_match,
                         position,
                         len(self.text),
                        )
        # Weights computed using logistic regression
        weights = (
                   1.8214,  # page_cosine_sim
                   0.4712,  # term_count
                   0.4206,  # related_sum
                   1.7482,  # related_average
                   0.187,  # causal_match
                   - 0.0559,  # position
                   - 0.0002,  # text_length
                   )
        score = -13.0476  # intercept
        for w, x in zip(weights, self._features):
            # Multiply each feature by its corresponding weight.
            score += w * x
        # Compute probability using the sigmoid function.
        score = 1 / (math.exp(-score) + 1)

        return score

    def _compute_score(self, raw_tokens, ans_eng):
        """Compute various score components from the answer text.

        Args:
            raw_tokens: List of token strings, such as from a sentence.
            ans_eng: The AnswerEngine object that was used to generate
                this Answer object.

        Returns:
            term_count: Integer number of how many query-terms had a
                matching answer-term with a semantic relatedness (LCH)
                value above the threshold specified in the ans_eng.
            related: List of LCH similarity values (semantic relatedness),
                representing the maximum LCH value for each query-term
                when evaluated with each answer-term.
            causal_match: Boolean value representing whether or not the
                (usually hidden) causal term had a match (LCH greater
                than the threshold in ans_eng).
            position: List of token indexes for each query term,
                representing the location in the raw_tokens that had the
                maximal semantic relatedness (LCH) for each query term.
        """
        term_count = 0
        related = []
        causal_match = False
        position = []
        for term, synsets in ans_eng.ir_query_tagged:
            match = False
            term_related = []
            for i, page_term in enumerate(indexer.regularize(raw_tokens)):
                page_term_related = ans_eng.related_values(synsets, page_term)
                if page_term_related:
                    term_related.append((max(page_term_related), i))
                    if term == page_term or max(page_term_related) >= ans_eng.lch:
                        match = True
            if match:  # above LCH value
                term_count += 1
                if term == 'cause':
                    causal_match = True
            if term_related:
                term_related.sort(key=lambda tup: tup[0])
                term_related, i = term_related[-1]  # maximum value
                related.append(term_related)
                position.append(i)
        return term_count, related, causal_match, position


def get_answers(ans_eng):
    """Used to retrieve answers from an AnswerEngine and the query synsets.

    This is a convenience method for the multiprocessing module, as that
    module is unable to pickle bound methods (methods belonging to class
    instance). Instead, this function can be used by providing the
    AnswerEngine as an argument.

    Args:
        ans_eng: The AnswerEngine object to call get_answers() on.

    Returns:
        A tuple containing the list of Answer objects provided by the
        AnswerEngine and a modified ir_query_tagged attribute. The
        modified ir_query_tagged is list of tuples that have a term and
        another tuple of synsets. The synsets tuple contains the name of
        the synset and the definition of the synset.
    """
    ans_eng.get_answers()
    ir_query_tagged = []
    for term, synsets in ans_eng.ir_query_tagged[:-1]:
        synsets = [(net.name, net.definition) for net in synsets]
        ir_query_tagged.append((term, synsets))
    return ans_eng.answers, ir_query_tagged
